[{"authors":["admin"],"categories":null,"content":"I joined TuSimple USA in June, 2021. We are actively hiring now. Ping me if you are interested in building self-driving trucks together with us!\nI was one of the founding team members of the Baidu Autonomous Driving Car project. I joined Baidu in 2014. Since Jan. 2016, I was a tech lead at Baidu ADT, recruited and built the map and localization team from scratch. Starting in 2020, I initiated and lead two exploratory pilot projects, pedestrian trajectory prediction and imitation/reinforcement learning-based decision and planning. Prior to joining Baidu, I was a research scientist in the media analysis group in NEC Labs America, Cupertino CA.\nMy research interests include Computer Vision, Robotics, Machine Learning, Simultaneous Localization and Mapping (SLAM).\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://songshiyu01.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I joined TuSimple USA in June, 2021. We are actively hiring now. Ping me if you are interested in building self-driving trucks together with us!\nI was one of the founding team members of the Baidu Autonomous Driving Car project. I joined Baidu in 2014. Since Jan. 2016, I was a tech lead at Baidu ADT, recruited and built the map and localization team from scratch. Starting in 2020, I initiated and lead two exploratory pilot projects, pedestrian trajectory prediction and imitation/reinforcement learning-based decision and planning.","tags":null,"title":"宋适宇","type":"authors"},{"authors":["Lei He","Shengjie Jiang","Xiaoqing Liang","Ning Wang","**Shiyu Song**"],"categories":null,"content":"","date":1626220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626220800,"objectID":"adbdf6eca0a73384926ee4f6221755ec","permalink":"https://songshiyu01.github.io/publication/arxiv2021_diffnet/","publishdate":"2021-07-14T00:00:00Z","relpermalink":"/publication/arxiv2021_diffnet/","section":"publication","summary":"Up-to-date High-Definition (HD) maps are essential for self-driving cars. To achieve constantly updated HD maps, we present a deep neural network (DNN), Diff-Net, to detect changes in them. Compared to traditional methods based on object detectors, the essential design in our work is a parallel feature difference calculation structure that infers map changes by comparing features extracted from the camera and rasterized images. To generate these rasterized images, we project map elements onto images in the camera view, yielding meaningful map representations that can be consumed by a DNN accordingly. As we formulate the change detection task as an object detection problem, we leverage the anchor-based structure that predicts bounding boxes with different change status categories. Furthermore, rather than relying on single frame input, we introduce a spatio-temporal fusion module that fuses features from history frames into the current, thus improving the overall performance. Finally, we comprehensively validate our method's effectiveness using freshly collected datasets. Results demonstrate that our Diff-Net achieves better performance than the baseline methods and is ready to be integrated into a map production pipeline maintaining an up-to-date HD map.","tags":["List"],"title":"Diff-Net: Image Feature Difference based High-Definition Map Change Detection","type":"publication"},{"authors":["Jinyun Zhou","Rui Wang","Xu Liu","Yifei Jiang","Shu Jiang","Jiaming Tao","Jinghao Miao","**Shiyu Song**"],"categories":null,"content":"","date":1614643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"a540110ffbc06098b9b7ba01f1531f2c","permalink":"https://songshiyu01.github.io/publication/arxiv2021_planner/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/publication/arxiv2021_planner/","section":"publication","summary":"We present a learning-based planner that aims to robustly drive a vehicle by mimicking human drivers' driving behavior. We leverage a mid-to-mid approach that allows us to manipulate the input to our imitation learning network freely. With that in mind, we propose a novel feedback synthesizer for data augmentation. It allows our agent to gain more driving experience in various previously unseen environments that are likely to encounter, thus improving overall performance. This is in contrast to prior works that rely purely on random synthesizers. Furthermore, rather than completely commit to imitating, we introduce task losses that penalize undesirable behaviors, such as collision, off-road, and so on. Unlike prior works, this is done by introducing a differentiable vehicle rasterizer that directly converts the waypoints output by the network into images. This effectively avoids the usage of heavyweight ConvLSTM networks, therefore, yields a faster model inference time. About the network architecture, we exploit an attention mechanism that allows the network to reason critical objects in the scene and produce better interpretable attention heatmaps. To further enhance the safety and robustness of the network, we add an optional optimization-based post-processing planner improving the driving comfort. We comprehensively validate our method's effectiveness in different scenarios that are specifically created for evaluating self-driving vehicles. Results demonstrate that our learning-based planner achieves high intelligence and can handle complex situations. Detailed ablation and visualization analysis are included to further demonstrate each of our proposed modules' effectiveness in our method.","tags":["List"],"title":"Exploring Imitation Learning for Autonomous Driving with Feedback Synthesizer and Differentiable Rasterization","type":"publication"},{"authors":["Shiyu Song"],"categories":null,"content":"","date":1600203600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600203600,"objectID":"99a78713ee02772a1417b83076c5b261","permalink":"https://songshiyu01.github.io/talk/baiduworld2020/","publishdate":"2020-09-15T14:00:00-07:00","relpermalink":"/talk/baiduworld2020/","section":"talk","summary":"We make the introduction of several key techniques in our self-driving car, including a multi-sensor fusion based localization system, a localization system aided by odometry, and two learning-based methods using both LiDAR or vision sensors.","tags":[],"title":"Towards Learning-based Localization for Autonomous Driving (Chinese)","type":"talk"},{"authors":["Yao Zhou","Guowei Wan","Shenhua Hou","Li Yu","Gang Wang","Xiaofei Rui","**Shiyu Song**"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"921184858e7e53ee837595e3cf7b5527","permalink":"https://songshiyu01.github.io/publication/arxiv2020_visloc/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/arxiv2020_visloc/","section":"publication","summary":"We present a visual localization framework aided by novel deep attention aware features for autonomous driving that achieves centimeter level localization accuracy. Conventional approaches to the visual localization problem rely on handcrafted features or human-made objects on the road. They are known to be either prone to unstable matching caused by severe appearance or lighting changes, or too scarce to deliver constant and robust localization results in challenging scenarios. In this work, we seek to exploit the deep attention mechanism to search for salient, distinctive and stable features that are good for long-term matching in the scene through a novel end-to-end deep neural network. Furthermore, our learned feature descriptors are demonstrated to be competent to establish robust matches and therefore successfully estimate the optimal camera poses with high precision. We comprehensively validate the effectiveness of our method using a freshly collected dataset with high-quality ground truth trajectories and hardware synchronization between sensors. Results demonstrate that our method achieves a competitive localization accuracy when compared to the LiDAR-based localization solutions under various challenging circumstances, leading to a potential low-cost localization solution for autonomous driving.","tags":[],"title":"DA4AD: End-to-end Deep Attention Aware Features Aided Visual Localization for Autonomous Driving","type":"publication"},{"authors":["Yao Zhou","Guowei Wan","Shenhua Hou","Li Yu","Gang Wang","Xiaofei Rui","**Shiyu Song**"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"f2aefcbb0a1d9b298714bb6f0c58db4c","permalink":"https://songshiyu01.github.io/publication/eccv2020_visloc/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/eccv2020_visloc/","section":"publication","summary":"We present a visual localization framework based on novel deep attention aware features for autonomous driving that achieves centimeter level localization accuracy. Conventional approaches to the visual localization problem rely on handcrafted features or human-made objects on the road. They are known to be either prone to unstable matching caused by severe appearance or lighting changes, or too scarce to deliver constant and robust localization results in challenging scenarios. In this work, we seek to exploit the deep attention mechanism to search for salient, distinctive and stable features that are good for long-term matching in the scene through a novel end-to-end deep neural network. Furthermore, our learned feature descriptors are demonstrated to be competent to establish robust matches and therefore successfully estimate the optimal camera poses with high precision. We comprehensively validate the effectiveness of our method using a freshly collected dataset with high-quality ground truth trajectories and hardware synchronization between sensors. Results demonstrate that our method achieves a competitive localization accuracy when compared to the LiDAR-based localization solutions under various challenging circumstances, leading to a potential low-cost localization solution for autonomous driving.","tags":["List"],"title":"DA4AD: End-to-end Deep Attention-based Visual Localization for Autonomous Driving","type":"publication"},{"authors":["Wendong Ding","Shenhua Hou","Hang Gao","Guowei Wan","**Shiyu Song**"],"categories":null,"content":"","date":1579507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579507200,"objectID":"5695637b0165c7d3ed2071dfc8fd8397","permalink":"https://songshiyu01.github.io/publication/icra2020_lio/","publishdate":"2020-01-20T00:00:00-08:00","relpermalink":"/publication/icra2020_lio/","section":"publication","summary":"Environmental fluctuations pose crucial challenges to a localization system in autonomous driving. We present a robust LiDAR localization system that maintains its kinematic estimation in changing urban scenarios by using a dead reckoning solution implemented through a LiDAR inertial odometry. Our localization framework jointly uses information from complementary modalities such as global matching and LiDAR inertial odometry to achieve accurate and smooth real time localization estimation. To improve the performance of the LiDAR odometry, we incorporate inertial and LiDAR intensity cues into an occupancy grid based LiDAR odometry to enhance frame-to-frame motion and matching estimation. Multi-resolution occupancy grid is implemented yielding a coarse-to-fine approach to balance the odometry's precision and computational requirement. To fuse both the odometry and global matching results, we formulate a MAP estimation problem in a pose graph fusion framework that can be efficiently solved. An effective environmental change detection method is proposed that allows us to know exactly when and what portion of the map requires an update. We comprehensively validate the effectiveness of the proposed approaches using both the Apollo SouthBay dataset and our internal dataset. The results confirm that our efforts lead to a more robust and accurate localization system, especially in dynamically changing urban scenarios, outperforming state-of-the-art approaches.","tags":["List"],"title":"LiDAR Inertial Odometry Aided Robust LiDAR Localization System in Changing City Scenes","type":"publication"},{"authors":["Shiyu Song"],"categories":null,"content":"","date":1560787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560787200,"objectID":"d2a70ef2fb8054f480715815d3e5c82b","permalink":"https://songshiyu01.github.io/talk/cvpr2019/","publishdate":"2019-06-17T09:00:00-07:00","relpermalink":"/talk/cvpr2019/","section":"talk","summary":"We make a brief introduction of the techniques behind our multi-sensor fusion based localization system. We present a robust and precise localization system that achieves centimeter-level localization accuracy in disparate city scenes. The system adaptively uses information from complementary sensors such as GNSS, LiDAR and IMU to achieve high localization accuracy and resilience in challenging scenes, such as urban downtown, highways, and tunnels. In this tutorial, we introduce the technical principles of each individual localization method and the multisensor fusion framework. We also cover our latest work in the exploration of the learning based LiDAR localization method. It's good for engineers and Ph.D. students who are interested in the vehicle localization system.","tags":[],"title":"Inside Apollo: Multisensor Fusion Based Localization","type":"talk"},{"authors":["Weixin Lu","Guowei Wan","Yao Zhou","Xiangyu Fu","Pengfei Yuan","**Shiyu Song**"],"categories":null,"content":"","date":1560668400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560668400,"objectID":"f30c4df6c24f2157fa21fa8268359e7c","permalink":"https://songshiyu01.github.io/publication/iccv2019_registration/","publishdate":"2019-06-16T00:00:00-07:00","relpermalink":"/publication/iccv2019_registration/","section":"publication","summary":"We present DeepVCP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the interference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable registration accuracy and runtime efficiency to the state-of-the-art geometry-based methods, but with higher robustness to inaccurate initial poses. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method make it attractive to the substantial applications relying on the point cloud registration task.","tags":["List"],"title":"DeepVCP: An End-to-End Deep Neural Network for Point Cloud Registration","type":"publication"},{"authors":["Weixin Lu","Guowei Wan","Yao Zhou","Xiangyu Fu","Pengfei Yuan","**Shiyu Song**"],"categories":null,"content":"","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"a9c0b8bce3b12ee840ca76c78f520866","permalink":"https://songshiyu01.github.io/publication/arxiv2019_registration/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/publication/arxiv2019_registration/","section":"publication","summary":"We present DeepICP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the inference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. Our loss function incorporates both the local similarity and the global geometric constraints to ensure all above network designs can converge towards the right direction. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable or better performance than the state-of-the-art geometry-based methods. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method makes it attractive for substantial applications relying on the point cloud registration task.","tags":[],"title":"DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration","type":"publication"},{"authors":["Shiyu Song"],"categories":null,"content":"","date":1553788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553788800,"objectID":"3f53f6a4cc7cb01ff941fd1eb4bfbabc","permalink":"https://songshiyu01.github.io/talk/mipr2019/","publishdate":"2019-03-28T09:00:00-07:00","relpermalink":"/talk/mipr2019/","section":"talk","summary":"We present a robust and precise localization system that achieves centimeter-level localization accuracy in varied city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and robustness in various challenging scenes, including urban downtown, highway, tunnel and so on. Moving forward, we introduce our latest work in exploring learning-based localization system. It leverages the help of the deep neural network structures to establish a learning-based approach that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines.","tags":[],"title":"Towards Learning-based Localization for Autonomous Driving","type":"talk"},{"authors":["Weixin Lu","Yao Zhou","Guowei Wan","Shenhua Hou","**Shiyu Song**"],"categories":null,"content":"","date":1550304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550304000,"objectID":"73891adf2dc74faa0824701c122a911b","permalink":"https://songshiyu01.github.io/publication/cvpr2019_localization/","publishdate":"2019-02-16T00:00:00-08:00","relpermalink":"/publication/cvpr2019_localization/","section":"publication","summary":"We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The  SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation.","tags":["List"],"title":"L3-Net: Towards Learning based LiDAR Localization for Autonomous Driving","type":"publication"},{"authors":["Guowei Wan","Xiaolong Yang","Renlan Cai","Hao Li","Yao Zhou","Hao Wang","**Shiyu Song**"],"categories":null,"content":"","date":1526886000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526886000,"objectID":"e43ca0370610bcda7aa22f0e55aded83","permalink":"https://songshiyu01.github.io/publication/icra2018_localization/","publishdate":"2018-05-21T00:00:00-07:00","relpermalink":"/publication/icra2018_localization/","section":"publication","summary":"We present a robust and precise localization system that achieves centimeter-level localization accuracy in disparate city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and resilience in challenging scenes, such as urban downtown, highways, and tunnels. Rather than relying only on LiDAR intensity or 3D geometry, we make innovative use of LiDAR intensity and altitude cues to significantly improve localization system accuracy and robustness. Our GNSS RTK module utilizes the help of the multi-sensor fusion framework and achieves a better ambiguity resolution success rate. An error-state Kalman filter is applied to fuse the localization measurements from different sources with novel uncertainty estimation. We validate, in detail, the effectiveness of our approaches, achieving 5-10cm RMS accuracy and outperforming previous state-of-the-art systems. Importantly, our system, while deployed in a large autonomous driving fleet, made our vehicles fully autonomous in crowded city streets despite road construction that occurred from time to time. A dataset including more than 60 km real traffic driving in various urban roads is used to comprehensively test our system.","tags":["List"],"title":"Robust and Precise Vehicle Localization based on Multi-sensor Fusion in Diverse City Scenes","type":"publication"},{"authors":["Shiyu Song"],"categories":null,"content":"","date":1522252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522252800,"objectID":"8c0ed1ac311006869dbf0d9d7dc58950","permalink":"https://songshiyu01.github.io/talk/baiduai2018/","publishdate":"2018-03-28T09:00:00-07:00","relpermalink":"/talk/baiduai2018/","section":"talk","summary":"We make brief introduction of HD Map services at Baidu Apollo platform and the techniques behind our multi-sensor fusion based localization system.","tags":[],"title":"Localization and HD Map at Baidu IDG","type":"talk"},{"authors":["Guowei Wan","Xiaolong Yang","Renlan Cai","Hao Li","Hao Wang","**Shiyu Song**"],"categories":null,"content":"","date":1510704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510704000,"objectID":"70fdf56c3712678e988f052f5b942a7b","permalink":"https://songshiyu01.github.io/publication/arxiv2017_localization/","publishdate":"2017-11-15T00:00:00Z","relpermalink":"/publication/arxiv2017_localization/","section":"publication","summary":"We present a robust and precise localization system that achieves centimeter-level localization accuracy in disparate city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and resilience in challenging scenes, such as urban downtown, highways, and tunnels. Rather than relying only on LiDAR intensity or 3D geometry, we make innovative use of LiDAR intensity and altitude cues to significantly improve localization system accuracy and robustness. Our GNSS RTK module utilizes the help of the multi-sensor fusion framework and achieves a better ambiguity resolution success rate. An error-state Kalman filter is applied to fuse the localization measurements from different sources with novel uncertainty estimation. We validate, in detail, the effectiveness of our approaches, achieving 5-10cm RMS accuracy and outperforming previous state-of-the-art systems. Importantly, our system, while deployed in a large autonomous driving fleet, made our vehicles fully autonomous in crowded city streets despite road construction that occurred from time to time. A dataset including more than 60 km real traffic driving in various urban roads is used to comprehensively test our system.","tags":[],"title":"Robust and Precise Vehicle Localization based on Multi-sensor Fusion in Diverse City Scenes","type":"publication"},{"authors":["Shiyu Song"],"categories":null,"content":"","date":1505836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505836800,"objectID":"e394d4612a32e2833cb299bc17eac536","permalink":"https://songshiyu01.github.io/talk/isprs2017/","publishdate":"2017-09-19T09:00:00-07:00","relpermalink":"/talk/isprs2017/","section":"talk","summary":"We make brief introduction of the development history of Baidu self-driving car, Baidu Apollo platform for developers, the techniques behind the Baidu HD Map and our multi-sensor fusion based localization system. We present a robust and precise localization system that achieves centimeter-level localization accuracy in varied city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and robustness in various challenging scenes, including urban downtown, highway, tunnel and so on. Both our HD Map products and localization system have been deployed in a large autonomous driving fleet, and make our vehicles fully autonomous in crowded city streets every day.","tags":[],"title":"HD Map, Localization and Self-Driving Car","type":"talk"},{"authors":["**Shiyu Song**","Manmohan Chandraker","Clark C. Guest"],"categories":null,"content":"","date":1459494000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459494000,"objectID":"8222257a391e42a7b036b6427b4d9fac","permalink":"https://songshiyu01.github.io/publication/pami2016_monovo/","publishdate":"2016-04-01T00:00:00-07:00","relpermalink":"/publication/pami2016_monovo/","section":"publication","summary":"We present a real-time monocular visual odometry system that achieves high accuracy in real-world autonomous driving applications. First, we demonstrate robust monocular SFM that exploits multithreading to handle driving scenes with large motions and rapidly changing imagery. To correct for scale drift, we use known height of the camera from the ground plane. Our second contribution is a novel data-driven mechanism for cue combination that allows highly accurate ground plane estimation by adapting observation covariances of multiple cues, such as sparse feature matching and dense inter-frame stereo, based on their relative confidences inferred from visual data on a per-frame basis. Finally, we demonstrate extensive benchmark performance and comparisons on the challenging KITTI dataset, achieving accuracy comparable to stereo and exceeding prior monocular systems. Our SFM system is optimized to output pose within 50 ms in the worst case, while average case operation is over 30 fps. Our framework also significantly boosts the accuracy of applications like object localization that rely on the ground plane.","tags":["List"],"title":"High Accuracy Monocular SFM and Scale Correction for Autonomous Driving","type":"publication"},{"authors":null,"categories":null,"content":"Summary We present a robust and precise localization system that achieves centimeter-level localization accuracy in disparate city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and resilience in challenging scenes, such as urban downtown, highways, and tunnels. Rather than relying only on LiDAR intensity or 3D geometry, we make innovative use of LiDAR intensity and altitude cues to significantly improve localization system accuracy and robustness. Our GNSS RTK module utilizes the help of the multi-sensor fusion framework and achieves a better ambiguity resolution success rate. An error-state Kalman filter is applied to fuse the localization measurements from different sources with novel uncertainty estimation. We validate, in detail, the effectiveness of our approaches, achieving 5-10cm RMS accuracy and outperforming previous state-of-the-art systems. Importantly, our system, while deployed in a large autonomous driving fleet, made our vehicles fully autonomous in crowded city streets despite road construction that occurred from time to time. A dataset including more than 60 km real traffic driving in various urban roads is used to comprehensively test our system.\nSensors Our autonomous vehicle is equipped with a Velodyne LiDAR HDL-64E. An integrated navigation system, NovAtel ProPak6 plus NovAtel IMU-IGM-A1, is installed for raw sensor data collection, such as GNSS pseudo range and carrier wave, IMU specific force and rotation rate. The built-in tightly integrated inertial and satellite navigation solution was not used. A computing platform equipped with Dual Xeon E5-2658 v3 12 cores, and a Xilinx KU115 FPGA chip with 55% utilization for LiDAR localization.\nFramework Overview of the architecture of our system that estimates the optimal position, velocity, attitude (PVA) of the autonomous vehicle using a loosely coupled error-state-Kalman filter. It combines sensor input (purple) with pre-built LiDAR map (yellow). GNSS and LiDAR estimate the PVA used by an error-state Kalman filter as the measurements, while the Kalman filter provides the predicted prior PVA. The strap-down inertial navigation system (SINS) is used as a prediction model in the Kalman filter propagation phase by integrating the specific force $f^b$ measured by the accelerometer and the rotation rate $\\omega_{ib}^b$ measured by the gyroscope. The corrections including the bias of accelerometer and gyroscope, the errors of PVA, etc estimated by the Kalman filter are fed to the SINS.\nAccuracy Our system has been extensively tested in real-world driving scenarios. We compare our localization performance against the state-of-the-art intensity-based localization method proposed by Levinson et al. [12] and the built-in tightly-coupled GNSS/IMU integrated solution in the commercial product. In order to explicitly demonstrate the contribution of different sensors, the test results of our proposed system are shown in two modes:\n 2-Systems: LiDAR + IMU 3-Systems: LiDAR + GNSS + IMU.  In the table, we show the quantitative results in both regular or weak GNSS roads. Note our vast performance improvement over [12] and the robust and accurate localization results in both regular and weak GNSS scenarios with centimeter level accuracy.\nVideos These videos provide a brief description of the project and demonstrate the performance of the localization system.\n  ","date":1446451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446451200,"objectID":"1ed2ad44af3b68ef41bff90310904192","permalink":"https://songshiyu01.github.io/project/msf_localization/","publishdate":"2015-11-02T00:00:00-08:00","relpermalink":"/project/msf_localization/","section":"project","summary":"A robust and precise vehicle localization system that achieves centimeter-level accuracy by adaptively fusing information from multiple complementary sensors, such as GNSS, LiDAR, camera and IMU, for self-driving cars.","tags":[],"title":"Multi-sensor Fusion based Localization System","type":"project"},{"authors":["**Shiyu Song**","Manmohan Chandraker"],"categories":null,"content":"","date":1433746800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433746800,"objectID":"ee736fb4cd1dc4c981028c1136d44b52","permalink":"https://songshiyu01.github.io/publication/cvpr2015_object/","publishdate":"2015-06-08T00:00:00-07:00","relpermalink":"/publication/cvpr2015_object/","section":"publication","summary":"We present a system for fast and highly accurate 3D localization of objects like cars in autonomous driving applications, using a single camera. Our localization framework jointly uses information from complementary modalities such as structure from motion (SFM) and object detection to achieve high localization accuracy in both near and far fields. This is in contrast to prior works that rely purely on detector outputs, or motion segmentation based on sparse feature tracks. Rather than completely commit to tracklets generated by a 2D tracker, we make novel use of raw detection scores to allow our 3D bounding boxes to adapt to better quality 3D cues. To extract SFM cues, we demonstrate the advantages of dense tracking over sparse mechanisms in autonomous driving scenarios. In contrast to complex scene understanding, our formulation for 3D localization is efficient and can be regarded as an extension of sparse bundle adjustment to incorporate object detection cues. Experiments on the KITTI dataset show the efficacy of our cues, as well as the accuracy and robustness of our 3D object localization relative to ground truth and prior works.","tags":["List"],"title":"Joint SFM and Detection Cues for Monocular 3D Localization in Road Scenes","type":"publication"},{"authors":["**Shiyu Song**","Manmohan Chandraker"],"categories":null,"content":"","date":1403593200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1403593200,"objectID":"a835ccaaf5aa697a0a0715fe2f30592a","permalink":"https://songshiyu01.github.io/publication/cvpr2014_monovo/","publishdate":"2014-06-24T00:00:00-07:00","relpermalink":"/publication/cvpr2014_monovo/","section":"publication","summary":"Scale drift is a crucial challenge for monocular autonomous driving to emulate the performance of stereo. This paper presents a real-time monocular SFM system that corrects for scale drift using a novel cue combination framework for ground plane estimation, yielding accuracy comparable to stereo over long driving sequences. Our ground plane estimation uses multiple cues like sparse features, dense inter-frame stereo and (when applicable) object detection. A data-driven mechanism is proposed to learn models from training data that relate observation covariances for each cue to error behavior of its underlying variables. During testing, this allows per-frame adaptation of observation covariances based on relative confidences inferred from visual data. Our framework significantly boosts not only the accuracy of monocular self-localization, but also that of applications like object localization that rely on the ground plane. Experiments on the KITTI dataset demonstrate the accuracy of our ground plane estimation, monocular SFM and object localization relative to ground truth, with detailed comparisons to prior art.","tags":["List"],"title":"Robust Scale Estimation in Real-Time Monocular SFM for Autonomous Driving","type":"publication"},{"authors":["**Shiyu Song**","Manmohan Chandraker","Clark C. Guest"],"categories":null,"content":"","date":1367823600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367823600,"objectID":"29fa1678e11ba90da425efaf83a0c644","permalink":"https://songshiyu01.github.io/publication/icra2013_monovo/","publishdate":"2013-05-06T00:00:00-07:00","relpermalink":"/publication/icra2013_monovo/","section":"publication","summary":"We present a real-time, accurate, large-scale monocular visual odometry system for real-world autonomous outdoor driving applications. The key contributions of our work are a series of architectural innovations that address the challenge of robust multithreading even for scenes with large motions and rapidly changing imagery. Our design is extensible for three or more parallel CPU threads. The system uses 3D-2D correspondences for robust pose estimation across all threads, followed by local bundle adjustment in the primary thread. In contrast to prior work, epipolar search operates in parallel in other threads to generate new 3D points at every frame. This significantly boosts robustness and accuracy, since only extensively validated 3D points with long tracks are inserted at keyframes. Fast-moving vehicles also necessitate immediate global bundle adjustment, which is triggered by our novel keyframe design in parallel with pose estimation in a thread-safe architecture. To handle inevitable tracking failures, a recovery method is provided. Scale drift is corrected only occasionally, using a novel mechanism that detects (rather than assumes) local planarity of the road by combining information from triangulated 3D points and the inter-image planar homography. Our system is optimized to output pose within 50 ms in the worst case, while average case operation is over 30 fps. Evaluations are presented on the challenging KITTI dataset for autonomous driving, where we achieve better rotation and translation accuracy than other state-of-the-art systems.","tags":["List"],"title":"Parallel, Real-Time Monocular Visual Odometry","type":"publication"},{"authors":null,"categories":null,"content":"Summary Scale drift is a crucial challenge for monocular autonomous driving to emulate the performance of stereo. This paper presents a real-time monocular SFM system that corrects for scale drift using a novel cue combination framework for ground plane estimation, yielding accuracy comparable to stereo over long driving sequences. Our ground plane estimation uses multiple cues like sparse features, dense inter-frame stereo and (when applicable) object detection. A data-driven mechanism is proposed to learn models from training data that relate observation covariances for each cue to error behavior of its underlying variables. During testing, this allows per-frame adaptation of observation covariances based on relative confidences inferred from visual data. Our framework significantly boosts not only the accuracy of monocular self-localization, but also that of applications like object localization that rely on the ground plane. Experiments on the KITTI dataset demonstrate the accuracy of our ground plane estimation, monocular SFM and object localization relative to ground truth, with detailed comparisons to prior art.\nAccuracy We demonstrate our performance on the KITTI dataset. For camera self-localization, our purely vision-based system achieves a rotation error of 0.005 degrees per meter and a translation error of 2.5%, which compares favorably even to state-of-the-art stereo systems and significantly outperforms other monocular systems. For 3D localization of other traffic participants like cars, we achieve low errors of 8% for near objects (within 30 meters) and 12% for far objects (beyond 30 meters).\nCue combination The main challenge in monocular SFM is scale drift, since unlike the case of stereo, there is no reference baseline. We overcome this challenge with a novel cue combination framework, that combines information from 3D points, inter-frame stereo and object detection.\nComparison to other systems Our real-time monocular SFM is comparable in accuracy to state-of-the-art stereo systems and significantly outperforms other monocular systems. A few example sequences are shown here from the KITTI benchmark.\nVideos These videos provide a 1-minute description of the paper and demonstrate the monocular visual odometry and 3D object localization results.\nOverview   3D object localization   Monocular visual odometry   ","date":1329552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1329552000,"objectID":"0ef0ca75b917dbadfe809e39d8154bc4","permalink":"https://songshiyu01.github.io/project/monocular_vo/","publishdate":"2012-02-18T00:00:00-08:00","relpermalink":"/project/monocular_vo/","section":"project","summary":"A real-time monocular visual odometry system that corrects for scale drift using a novel cue combination framework for ground plane estimation, yielding accuracy comparable to stereo over long driving sequences.","tags":[],"title":"Monocular Visual Odometry","type":"project"}]